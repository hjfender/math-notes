\subsection{Homomorphisms of free modules}

If $F$ is a free module, then there is a set $A$ determined up to bijection/choice-of-basis (see \ref{basisisfree}) such that $F \cong R^{\oplus A}$.

So we can understand Hom$_R(F_1, F_2)$ in terms of Hom$_R(R^{\oplus A_1}, R^{\oplus A_2})$ up to the choice of isomorphisms $F_1 \cong R^{\oplus A_1}$
and $F_2 \cong R^{\oplus A_2}$ (i.e. choice of basis.) In the case that $A_1$ and $A_2$ are finite we can simply describe Hom$_R(R^n, R^m)$ as an $R$-module,
for every choice of $m,n \in \mathbb{Z}$ in order to understand thes morphisms. This can be done with matrices with entries in $R$.

\subsubsection{Matrices}\label{matrices}
An $m \times n$ \emph{matrix} with entries in $R$ is a choice of $mn$ elements in $R$. It is commonly denoted as\dots
\[
(r_{ij})_{i=1,\dots,m;\,j=1,\dots,n} =
\begin{pmatrix}
	r_{11} & r_{12} & \cdots & r_{1n} \\
	r_{21} & r_{22} & \cdots & r_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	r_{m1} & r_{m2} & \cdots & r_{mn}
\end{pmatrix}
\]

The set $\mathcal{M}_{m,n}(R)$ of $m \times n$ matrices with entries in $R$ is an $R$-module under entrywise addition\dots
$$(a_{ij}) + (b_{ij}) := (a_{ij} + b_{ij})$$
and the action\dots
$$r(a_{ij}) := (ra_{ij})$$
for $r \in R$.

We can also define multiplication between $m \times p$ matrices and $p \times n$ matrices\dots
$$A_{m \times p} \cdot B_{p \times n} = (a_{ik}) \cdot (b_{kj}) := \left( \sum_{k=1}^{p} a_{ik} b_{kj} \right).$$
This multiplication is associative.

Thus the set $\mathcal{M}_n(R)$ of $n \times n$ matrices with entries in $R$ is an $R$-algebra with the identity element\dots
\[
\begin{pmatrix}
	1 & 0 & \cdots & 0 \\
	0 & 1 & \cdots & 0 \\
	\vdots & \vdots & \ddots & \vdots \\
	0 & 0 & \cdots & 1
\end{pmatrix}
\]

A \label{vector} \emph{column $n$-vector} is a $n \times 1$ matrix and a \emph{row $m$-vector} is a $1 \times m$ matrix. These structures
can stand in for elements of $\textbf{v} \in R^n$ (or $R^m$ as the case may be). Thus we can act on $R^n$ with an $m \times n$ matrix, by
left-multiplication\dots
\[
A \cdot \textbf{v} =
\begin{pmatrix}
	a_{11} & a_{12} & \cdots & a_{1n} \\
	a_{21} & a_{22} & \cdots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}
\cdot
\begin{pmatrix}
	v_{1}\\
	v_{2}\\
	\vdots\\
	v_{n}
\end{pmatrix}
=
\begin{pmatrix}
	a_{11}v_1 + a_{12}v_2 + \cdots + a_{1n}v_n\\
	a_{21}v_1 + a_{22}v_2 + \cdots + a_{2n}v_n\\
	\vdots\\
	a_{m1}v_1 + a_{m2}v_2 + \cdots + a_{mn}v_n
\end{pmatrix}
\in R^m.
\]
\begin{lemma}
For all $m \times n$ matrices $A$ with entries in $R$:
\begin{itemize}
  \item The function $\varphi : R^n \rightarrow R^m$ defined by $\varphi(\textbf{v}) = A \cdot \textbf{v}$ is a
  homomorphism of $R$-modules.
  \item Every $R$-module homomorphism $R^n \rightarrow R^m$ is determined in this way by a unique $m \times n$ matrix.
\end{itemize}
\end{lemma}

\begin{corollary}
The correspondence introduced in the previous lemma gives an isomorphism of $R$-modules\dots
$$\mathcal{M}_{m,n}(R) \cong \emph{Hom}_R(R^n,R^m).$$
\end{corollary}

\begin{lemma}
The following diagram commutes\dots
\begin{figure}[H]
\centering
\input{topics/modules/diagrams/matrixcomposition}
\end{figure}
\end{lemma}

\subsubsection{Change of Basis}\label{changeofbasis}
Let $F$ be a finitely generated free module and choose two (finite) bases $A$, $B$ for $F$. Then the two bases correspond
to two isomorphisms\dots
$$R^{\oplus A} \xrightarrow[]{\varphi} F, \; R^{\oplus B} \xrightarrow[]{\psi} F.$$
Then\dots
$$R^{\oplus A} \xrightarrow[]{\psi^{-1} \circ \varphi} R^{\oplus B}$$
is an isomorphism, which corresponds to a matrix $N^B_A$, the \emph{matrix of the change of basis}.

\begin{proposition}
Let $\alpha : F \rightarrow G$ be a homomorphism of finitely generated free modules, and let $P$ be a matrix reperesenting
it with respect to any choice of bases for $F$ and $G$. Then the matrices representing $\alpha$ with respect to any other
choice of bases are all and only the matrices of the form\dots
$$M \cdot P \cdot N$$
where $M$ and $N$ are invertible matrices.

\subsubsection{Equivalent Matrices}\label{equivalentmatrices}
Two matrices $P, Q \in \mathcal{M}_{m,n}(R)$ are \emph{equivalent} if they represent the same homomorphism of free modules
$R^n \rightarrow R^m$ up to a choice of basis.
\end{proposition}

\subsubsection{Elementary Operations}
In this section we detail the elementary operations that can be performed on an $m \times n$ matrix $P$. The examples given are
to $4 \times 4$ invertible matrices that operate on the rows of an $4 \times n$ matrix. They should suggest an easy generalization to
larger invertible matrices and operations on columns.\newline

\noindent The Elementary operations on a matrix are\dots
\begin{itemize}
  \item switching two rows (or columns) of $P$;
  \textbf{Example.} \[
  	\begin{pmatrix}
		1 & 0 & 0 & 0 \\
		0 & 0 & 0 & 1 \\
		0 & 0 & 1 & 0 \\
		0 & 1 & 0 & 0
	\end{pmatrix}
  \] switches the second and fouth row
  \item add to one row (resp., column) a multiple of another row (resp., column);
  \textbf{Example:} \[
  	\begin{pmatrix}
		1 & 0 & c & 0 \\
		0 & 1 & 0 & 0 \\
		0 & 0 & 1 & 0 \\
		0 & 0 & 0 & 1
	\end{pmatrix}
  \] adds to the first row the $c$-multiple of the third column
  \item multiply all entries in one row (or column) of $P$ by a unit of $R$.
  \textbf{Example:} \[
  	\begin{pmatrix}
		1 & 0 & 0 & 0 \\
		0 & u & 0 & 0 \\
		0 & 0 & 1 & 0 \\
		0 & 0 & 0 & 1
	\end{pmatrix}
  \] multiplies the second row by the unit $u$
\end{itemize}

\begin{proposition}
Two matrices $P,Q \in \mathcal{M}_{m,n}(R)$ are equivalent if $Q$ may be obtained from $P$ by a sequence of elementary operations.
\end{proposition}

\subsubsection{Gaussian Elimination over Euclidean domains}\label{gaussianelimination}
We give a $2 \times 2$ example, which generalizes easily. Let\dots
\[
\begin{pmatrix}
	a & b\\
	c & d
\end{pmatrix}
\in \mathcal{M}_2(R),
\]
for a Euclidean domain $R$, with Euclidean valuation $N$. After switching rows and/or columns if necessary, we may assume that $N(a)$ is
the minimum of the valuations of all entries in the matrices. Division with remainder gives\dots
$b = aq + r$
with $r = 0$ or $N(r) < N(a)$. Adding to the second column the $(-q)$-multiple of the first produces the matrix\dots
\[
\begin{pmatrix}
	a & r\\
	c & d - qc
\end{pmatrix}.
\]
If $r \neq 0$, so that $N(r) < N(a)$, we begin again and shuffle rows and columns so that the ($1$,$1$) entry has minimum valuation. This
process may be repeated, but after a finite number of steps the ($1$,$2$) entry will have to vanish.

Trivial variations of the same procedure will clear the ($2$,$1$) entry as well, producing a matrix\dots
\[
\begin{pmatrix}
	e & 0\\
	0 & f
\end{pmatrix}.
\]
Now we may assume that $e$ divides $f$ in $R$, with no remainder. Indeed, otherwise we can add the second row to the first,
\[
\begin{pmatrix}
	e & f\\
	0 & f
\end{pmatrix},
\]
and start all over with this new matrix. Again, the effect of all the operations will be to decrease the valuation of the
($1,1$) entry, so after a final number of steps we must reach the condition $e | f$.

\begin{proposition}
Let $R$ be a Euclidean domain, and let $P \in \mathcal{M}_{m,n}(R)$. Then $P$ is equivalent to a matrix of the form\dots
\[
\begin{pmatrix}
	d_1 & \cdots & 0 & \rvline & 0 \\
	\vdots & \ddots & \vdots & \rvline & 0 \\
	0 & \cdots & d_r & \rvline & 0 \\ \hline
	0 & \cdots & 0 & \rvline & 0
\end{pmatrix},
\]
with $d_1 | \cdots | d_r$.

This is called the \emph{Smith normal form}\label{smithnormalform} of the matrix.
\end{proposition}

\subsubsection{The Determinant}\label{determinant}
Let $A = (a_{ij}) \in \mathcal{M}_n(R)$ be a square matrix. Then the \emph{determinant} of $A$ is the element\dots
$$\textrm{det}(A) = \sum_{\sigma \in S_n} (-1)^{\sigma} \prod^n_{i=1} a_{i \sigma(i)} \in R.$$

\noindent Some properties of the determinant are\dots
\begin{itemize}
  \item det$(A)=$ det$(A^t)$
  \item If two rows or columns of a matrix $A$ agree, then det$(A)=0$
  \item Suppose $A = (a_{ij})$ and $B = (b_{ij})$ agree on all but at most one row. Then det$(C)=$ det$(A)+$det$(B)$.
\end{itemize}

\begin{lemma}
Let $A$ be a square matrix with entries in an integral domain $R$.
\begin{itemize}
  \item Let $A'$ be obtained from A by switching two rows to two columns. Then \emph{det}$(A^t)=$ \emph{-det}$(A)$.
  \item Let $A'$ be obtained from A by adding to a row (resp. column) a multiple of another row (resp. column). Then \emph{det}$(A^t)=$ \emph{det}$(A)$.
  \item Let $A'$ be obtained from A by multiplying a row (resp. column) by an element $c \in R$. Then \emph{det}$(A')=c$\emph{det}$(A)$.
\end{itemize}
In other words, the effect of an elementary operation on \emph{det}$(A)$ is the same as multiplying \emph{det}$(A)$ by the determinant of the corresponding elementary matrix.
\end{lemma}

\subsubsection{Cofactors of a square matrix}\label{square}
If $A \in \mathcal{M}_n(R)$, the \emph{cofactors} of $A$ are the $(n-1) \times (m-1)$ minores of $A$, corrected by sign. More precisely, for $A = (a)_{ij}$, I will let\dots
\[
A^{ij} := (-1)^{i+j}\textrm{det}\begin{pmatrix}
									a_{11} & \cdots & a_{1j-1} & a_{1j+1} & \cdots & a_{1n}\\
									\vdots & \ddots & \vdots   & \vdots   & \ddots & \vdots\\
									a_{i-11} & \cdots & a_{i-1j-1} & a_{i-1j+1} & \cdots & a_{i-1n}\\
									a_{i+11} & \cdots & a_{i+1j-1} & a_{i+1j+1} & \cdots & a_{i+1n}\\
									\vdots & \ddots & \vdots   & \vdots   & \ddots & \vdots\\
									a_{n1} & \cdots & a_{nj-1} & a_{nj+1} & \cdots & a_{nn}
								\end{pmatrix}
\]

\begin{lemma}
With notation as above,
\begin{itemize}
  \item for all $i = 1, \dots, n$, det$(A) = \sum_{j=1}^n a_{ij}A^{(ij)}$,
  \item for all $j = 1, \dots, n$, det$(A) = \sum_{i=1}^n a_{ij}A^{(ij)}$.
\end{itemize}
\end{lemma}

\begin{corollary}
Let $R$ be a commutative ring and $A \in \mathcal{M}_n(R)$. Then\dots
\[
A \cdot \begin{pmatrix}
	a^{(11)} & \cdots & a^{(n1)}\\
	\vdots & \ddots & \vdots\\
	a^{(1n)} & \cdots & a^{(nn)}
\end{pmatrix} =
\begin{pmatrix}
	a^{(11)} & \cdots & a^{(n1)}\\
	\vdots & \ddots & \vdots\\
	a^{(1n)} & \cdots & a^{(nn)}
\end{pmatrix} \cdot A =
\emph{det}(A)\cdot I_n.
\]
\end{corollary}
\noindent The matrix of cofactors above is called the \emph{adjoint matrix} of $A$. Note\dots
\[
A^{-1} = \textrm{det}(A)^{-1}
\begin{pmatrix}
	a^{(11)} & \cdots & a^{(n1)}\\
	\vdots & \ddots & \vdots\\
	a^{(1n)} & \cdots & a^{(nn)}
\end{pmatrix}.
\]

\begin{proposition}[Cramer's rule]
\label{cramersrule}
Assume $\emph{det}(A)$ is a unite, and let $A^{(j)}$ be the matrix obtained by replacing the $j$-th column of $A$ by the
column vector $b$. Then\dots
$$x_j = \emph{det}(A)^{-1}\emph{det}(A^{(j)}).$$
\end{proposition}